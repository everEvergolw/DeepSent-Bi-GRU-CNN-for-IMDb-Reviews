{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1InzolKX-LYd"
      },
      "source": [
        "# **Import Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZRKY2btx-Xz_"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, GRU, Conv1D, GlobalMaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau \n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.metrics import f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iazMoJittnlv"
      },
      "source": [
        "# **Load the IMDb Dataset From a CSV File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf-zDvp4-f3x",
        "outputId": "3390ce45-35b5-446c-db72-407eea370398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-710c55cf3fd6>:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data = pd.read_csv(file_path, encoding='ISO-8859-1', error_bad_lines=False)\n"
          ]
        }
      ],
      "source": [
        "# Load the IMDb dataset from a CSV file\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/IMDB_Dataset.csv'\n",
        "data = pd.read_csv(file_path, encoding='ISO-8859-1', error_bad_lines=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua7w1WNb-r4e"
      },
      "source": [
        "# **Preprocess the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5WHwI2ibLRoA"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # 1. Remove HTML tags\n",
        "    clean_text = re.sub('<[^>]*>', '', text)\n",
        "\n",
        "    # 2. Remove special characters, URLs, and email addresses\n",
        "    clean_text = re.sub('[^\\w\\s]', ' ', clean_text)\n",
        "    clean_text = re.sub('\\S*@\\S*\\s?', '', clean_text)\n",
        "    clean_text = re.sub(r'http\\S+', '', clean_text)\n",
        "\n",
        "    # 3. Remove numbers\n",
        "    clean_text = re.sub('\\d+', '', clean_text)\n",
        "\n",
        "    # 4. Remove extra white space\n",
        "    clean_text = re.sub('\\s+', ' ', clean_text)\n",
        "\n",
        "    # 5. Remove punctuation and perform tokenization and Convert text to lowercase \n",
        "    tokenized_text = simple_preprocess(clean_text, deacc=True)\n",
        "\n",
        "    return tokenized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ei-2V7Y9-saC"
      },
      "outputs": [],
      "source": [
        "# Preprocess the dataset\n",
        "reviews = data['review']\n",
        "sentiments = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "\n",
        "# Preprocess and tokenize the reviews\n",
        "tokenized_reviews = [preprocess_text(review) for review in reviews]\n",
        "\n",
        "# Preprocess and tokenize the reviews\n",
        "# tokenized_reviews = [simple_preprocess(review) for review in reviews]\n",
        "\n",
        "# Create a tokenizer and fit it on the reviews\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "\n",
        "# Convert the tokenized reviews to sequences\n",
        "sequences = tokenizer.texts_to_sequences(reviews)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSoE-sxA_JkE"
      },
      "source": [
        "# **Train the Word2Vec Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bTNHvAUj_X93"
      },
      "outputs": [],
      "source": [
        "# Train the Word2Vec model with hyperparameters\n",
        "embedding_dim = 128\n",
        "word2vec_model = Word2Vec(sentences=tokenized_reviews, vector_size=embedding_dim, window=7, min_count=2, workers=4, sg=1, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10vv1k4J_cjk"
      },
      "source": [
        "# **Create an Embedding Matrix for the Embedding Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GM7HaiMr_hn_"
      },
      "outputs": [],
      "source": [
        "# Create an embedding matrix for the Embedding layer\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index < max_words:\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[index] = word2vec_model.wv[word]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nupRMgc5_pJc"
      },
      "source": [
        "# **Pad Sequences to Have the Same Length**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fijaBA8m_pUX"
      },
      "outputs": [],
      "source": [
        "# Pad sequences to have the same length\n",
        "maxlen = 500\n",
        "x = pad_sequences(sequences, maxlen=maxlen)\n",
        "y = np.array(sentiments)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLZTANwM_ssC"
      },
      "source": [
        "# **Split the Data into Training and Testing Sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QqZ5nZlR_s4e"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyH0NkOd_y5I"
      },
      "source": [
        "# **Define the Mbi-GRUMConv Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3jfjow7C_zCk"
      },
      "outputs": [],
      "source": [
        "# Define the Mbi-GRUMConv model with parameters\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(128, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4sdR9n5lzxe"
      },
      "source": [
        "\n",
        "\n",
        "# **Define the LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cLoC4VeVl3WP"
      },
      "outputs": [],
      "source": [
        "# Define the Bi-LSTM-Conv model with the updated parameters\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(128, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI_SOQ-u_4M1"
      },
      "source": [
        "# **Use Adam optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5bF8Ibnn_4Rt"
      },
      "outputs": [],
      "source": [
        "# Use Adam optimizer with a learning rate scheduler\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model with binary_crossentropy loss and accuracy metric\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxiAMUUw_8s9"
      },
      "source": [
        "# **Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoGyJeod_83H",
        "outputId": "360373bc-6b81-4010-c912-955d03833893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 500, 128)          1280000   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 500, 128)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 500, 256)         263168    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 500, 256)         1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 498, 128)          98432     \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,642,753\n",
            "Trainable params: 362,241\n",
            "Non-trainable params: 1,280,512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Train the model with early stopping and learning rate reduction\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "validation_split = 0.1\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLAS_DjnADMb"
      },
      "source": [
        "# **Fit the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NNxJu4CzADZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d599c985-a8eb-4205-da09-d795c0de4ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "563/563 [==============================] - 36s 64ms/step - loss: 0.5892 - accuracy: 0.8687 - val_loss: 0.5755 - val_accuracy: 0.8385 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "563/563 [==============================] - 34s 60ms/step - loss: 0.4629 - accuracy: 0.8839 - val_loss: 0.5464 - val_accuracy: 0.8390 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "563/563 [==============================] - 36s 63ms/step - loss: 0.4062 - accuracy: 0.8895 - val_loss: 0.3656 - val_accuracy: 0.8997 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "563/563 [==============================] - 34s 60ms/step - loss: 0.3777 - accuracy: 0.8901 - val_loss: 0.4237 - val_accuracy: 0.8655 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "563/563 [==============================] - 35s 63ms/step - loss: 0.3470 - accuracy: 0.8970 - val_loss: 0.3323 - val_accuracy: 0.9015 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "563/563 [==============================] - 35s 63ms/step - loss: 0.3348 - accuracy: 0.8970 - val_loss: 0.4559 - val_accuracy: 0.8425 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "562/563 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8985\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "563/563 [==============================] - 34s 61ms/step - loss: 0.3220 - accuracy: 0.8986 - val_loss: 0.4359 - val_accuracy: 0.8372 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "563/563 [==============================] - 35s 63ms/step - loss: 0.2861 - accuracy: 0.9131 - val_loss: 0.2808 - val_accuracy: 0.9160 - lr: 2.0000e-04\n",
            "Epoch 9/30\n",
            "563/563 [==============================] - 34s 61ms/step - loss: 0.2715 - accuracy: 0.9155 - val_loss: 0.2761 - val_accuracy: 0.9158 - lr: 2.0000e-04\n",
            "Epoch 10/30\n",
            "563/563 [==============================] - 35s 63ms/step - loss: 0.2640 - accuracy: 0.9174 - val_loss: 0.2701 - val_accuracy: 0.9158 - lr: 2.0000e-04\n",
            "Epoch 11/30\n",
            "563/563 [==============================] - 34s 60ms/step - loss: 0.2608 - accuracy: 0.9172 - val_loss: 0.2677 - val_accuracy: 0.9170 - lr: 2.0000e-04\n",
            "Epoch 12/30\n",
            "563/563 [==============================] - 35s 62ms/step - loss: 0.2535 - accuracy: 0.9211 - val_loss: 0.2653 - val_accuracy: 0.9202 - lr: 2.0000e-04\n",
            "Epoch 13/30\n",
            "563/563 [==============================] - 34s 60ms/step - loss: 0.2517 - accuracy: 0.9197 - val_loss: 0.2697 - val_accuracy: 0.9120 - lr: 2.0000e-04\n",
            "Epoch 14/30\n",
            "563/563 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.9213\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "563/563 [==============================] - 34s 61ms/step - loss: 0.2461 - accuracy: 0.9213 - val_loss: 0.2667 - val_accuracy: 0.9178 - lr: 2.0000e-04\n",
            "Epoch 15/30\n",
            "563/563 [==============================] - 36s 64ms/step - loss: 0.2347 - accuracy: 0.9267 - val_loss: 0.2589 - val_accuracy: 0.9175 - lr: 4.0000e-05\n",
            "Epoch 16/30\n",
            "563/563 [==============================] - 34s 61ms/step - loss: 0.2289 - accuracy: 0.9305 - val_loss: 0.2594 - val_accuracy: 0.9170 - lr: 4.0000e-05\n",
            "Epoch 17/30\n",
            "563/563 [==============================] - 34s 61ms/step - loss: 0.2266 - accuracy: 0.9302 - val_loss: 0.2581 - val_accuracy: 0.9172 - lr: 4.0000e-05\n",
            "Epoch 18/30\n",
            "563/563 [==============================] - 34s 61ms/step - loss: 0.2253 - accuracy: 0.9302 - val_loss: 0.2582 - val_accuracy: 0.9175 - lr: 4.0000e-05\n",
            "Epoch 19/30\n",
            "562/563 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9307\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "563/563 [==============================] - 34s 61ms/step - loss: 0.2247 - accuracy: 0.9307 - val_loss: 0.2627 - val_accuracy: 0.9145 - lr: 4.0000e-05\n",
            "Epoch 20/30\n",
            "563/563 [==============================] - 34s 61ms/step - loss: 0.2209 - accuracy: 0.9328 - val_loss: 0.2578 - val_accuracy: 0.9160 - lr: 8.0000e-06\n",
            "Epoch 21/30\n",
            "563/563 [==============================] - 36s 64ms/step - loss: 0.2194 - accuracy: 0.9335 - val_loss: 0.2576 - val_accuracy: 0.9165 - lr: 8.0000e-06\n",
            "Epoch 22/30\n",
            "563/563 [==============================] - 36s 63ms/step - loss: 0.2202 - accuracy: 0.9317 - val_loss: 0.2571 - val_accuracy: 0.9170 - lr: 8.0000e-06\n",
            "Epoch 23/30\n",
            "563/563 [==============================] - 36s 63ms/step - loss: 0.2220 - accuracy: 0.9327 - val_loss: 0.2568 - val_accuracy: 0.9170 - lr: 8.0000e-06\n",
            "Epoch 24/30\n",
            "563/563 [==============================] - 36s 63ms/step - loss: 0.2185 - accuracy: 0.9331 - val_loss: 0.2565 - val_accuracy: 0.9160 - lr: 8.0000e-06\n",
            "Epoch 25/30\n",
            "563/563 [==============================] - 34s 61ms/step - loss: 0.2192 - accuracy: 0.9327 - val_loss: 0.2569 - val_accuracy: 0.9170 - lr: 8.0000e-06\n",
            "Epoch 26/30\n",
            "563/563 [==============================] - ETA: 0s - loss: 0.2191 - accuracy: 0.9339\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "563/563 [==============================] - 36s 63ms/step - loss: 0.2191 - accuracy: 0.9339 - val_loss: 0.2565 - val_accuracy: 0.9162 - lr: 8.0000e-06\n",
            "Epoch 27/30\n",
            "563/563 [==============================] - 35s 62ms/step - loss: 0.2168 - accuracy: 0.9337 - val_loss: 0.2568 - val_accuracy: 0.9162 - lr: 1.6000e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9cf9b670d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Fit the model using the training data, with validation, and the specified callbacks\n",
        "checkpoint = ModelCheckpoint('M_model_weights_{epoch:02d}.h5', save_weights_only=False, save_freq='epoch')\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[early_stopping, reduce_lr,checkpoint])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZHzZovDpDYb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYW026yXANo6"
      },
      "source": [
        "# **Evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tNHR8AT3AN2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec54b37-d800-47c8-ce56-cc1524be23a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 4s 23ms/step - loss: 0.2484 - accuracy: 0.9171\n",
            "Test accuracy: 0.9171000123023987\n",
            "313/313 [==============================] - 5s 15ms/step\n",
            "F1 score: 0.9182848693937901\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "scores = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "# Print the test accuracy\n",
        "print(f\"Test accuracy: {scores[1]}\")\n",
        "\n",
        "# Get predictions for the test set\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Convert the predictions to binary by setting a threshold\n",
        "y_pred_binary = np.where(y_pred >= 0.5, 1, 0)\n",
        "\n",
        "# Compute and print the F1 score\n",
        "f1 = f1_score(y_test, y_pred_binary)\n",
        "print(f\"F1 score: {f1}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}